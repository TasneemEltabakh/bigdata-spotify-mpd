{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ccd27a-74e3-4a3b-a1d1-729054acc4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phase 3 – Business Insights Gold Layer\n",
    "\n",
    "This notebook builds business-ready GOLD insight tables for dashboarding, aligned with Phase 2 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c539c0a-137a-4514-9447-ec7016053218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5a9c5c-f2c9-4814-9704-82db68c07e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Phase 2 Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c95a8439-1d79-4aa7-8c48-442e228d87fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "silver_playlists = spark.table(\"silver_playlists\")\n",
    "silver_playlist_tracks = spark.table(\"silver_playlist_tracks\")\n",
    "\n",
    "gold_track_popularity = spark.table(\"gold_track_popularity\")\n",
    "gold_artist_popularity = spark.table(\"gold_artist_popularity\")\n",
    "gold_engagement_bins = spark.table(\"gold_engagement_bins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d22e65b-4492-4316-b8b2-68f981f41d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Executive KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b7cd645-311a-45b5-93a0-ff3e4923577c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gold_exec_kpis = (\n",
    "    silver_playlist_tracks\n",
    "    .join(silver_playlists, \"playlist_id\")\n",
    "    .groupBy()\n",
    "    .agg(\n",
    "        F.countDistinct(\"track_uri\").alias(\"total_tracks\"),\n",
    "        F.countDistinct(\"playlist_id\").alias(\"total_playlists\"),\n",
    "        F.countDistinct(\"artist_uri\").alias(\"total_artists\"),\n",
    "        F.avg(\"playlist_followers\").alias(\"avg_playlist_followers\"),\n",
    "        F.max(\"playlist_modified_at\").alias(\"last_data_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_exec_kpis.write.mode(\"overwrite\").saveAsTable(\"gold_exec_kpis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6986ccc-9484-4943-ae38-a8d9e00549ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## KPI Trend Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e068d0-e674-4533-a4c6-bec9e328fe00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gold_kpi_daily = (\n",
    "    silver_playlist_tracks\n",
    "    .groupBy(\"playlist_modified_at\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"track_uri\").alias(\"tracks_added\"),\n",
    "        F.countDistinct(\"playlist_id\").alias(\"active_playlists\")\n",
    "    )\n",
    "    .withColumnRenamed(\"playlist_modified_at\", \"event_date\")\n",
    "    .orderBy(\"event_date\")\n",
    ")\n",
    "\n",
    "gold_kpi_daily.write.mode(\"overwrite\").saveAsTable(\"gold_kpi_daily\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340c4822-5160-415a-bf70-761c93ce15fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Track Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d316b5-26fc-4ad3-8c82-a05e4ac0188e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spt = silver_playlist_tracks.alias(\"spt\")\n",
    "gtp = gold_track_popularity.alias(\"gtp\")\n",
    "\n",
    "gold_track_summary = (\n",
    "    spt\n",
    "    .join(gtp, \"track_uri\", \"left\")\n",
    "    .groupBy(\n",
    "        F.col(\"spt.track_uri\"),\n",
    "        F.col(\"spt.track_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        F.countDistinct(\"spt.playlist_id\").alias(\"playlist_count\"),\n",
    "        F.first(\"gtp.appearances\").alias(\"total_appearances\"),\n",
    "        F.first(\"gtp.playlists_count\").alias(\"playlists_count\"),\n",
    "        F.round(F.avg(\"spt.duration_ms\") / 60000, 2).alias(\"avg_duration_min\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_track_summary.write.mode(\"overwrite\").saveAsTable(\"gold_track_summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea22279b-5f21-4686-bd57-d624124d9601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Artist Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2276b50-fcb4-46f6-86b5-67cc02f4e1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spt = silver_playlist_tracks.alias(\"spt\")\n",
    "gap = gold_artist_popularity.alias(\"gap\")\n",
    "\n",
    "gold_artist_summary = (\n",
    "    spt\n",
    "    .join(\n",
    "        gap,\n",
    "        F.col(\"spt.artist_name\") == F.col(\"gap.artist_name\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .groupBy(\n",
    "        F.col(\"spt.artist_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        F.countDistinct(\"spt.track_uri\").alias(\"track_count\"),\n",
    "        F.first(\"gap.appearances\").alias(\"total_appearances\"),\n",
    "        F.first(\"gap.playlists_count\").alias(\"playlists_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_artist_summary.write.mode(\"overwrite\").saveAsTable(\"gold_artist_summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d978e43-6628-4422-a220-e21fbb47db6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Engagement Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ced9fec-ab4a-4a47-abb2-32fe816a52ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "gold_engagement_summary = (\n",
    "    gold_engagement_bins\n",
    "    .groupBy(\"tracks_bin\")\n",
    "    .agg(\n",
    "        F.sum(\"n_playlists\").alias(\"total_playlists\"),\n",
    "        F.round(F.avg(\"median_followers\"), 0).alias(\"avg_median_followers\"),\n",
    "        F.round(F.avg(\"p90_followers\"), 0).alias(\"avg_p90_followers\")\n",
    "    )\n",
    "    .orderBy(\"tracks_bin\")\n",
    ")\n",
    "\n",
    "gold_engagement_summary.write.mode(\"overwrite\").saveAsTable(\"gold_engagement_summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a7b360-f0b1-4f81-991a-d18eb1bfdbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Quality & Freshness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4528c3-170a-4a3b-ae18-aba1635624ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gold_data_quality = (\n",
    "    silver_playlist_tracks\n",
    "    .groupBy()\n",
    "    .agg(\n",
    "        F.max(\"playlist_modified_at\").alias(\"latest_snapshot\"),\n",
    "        F.countDistinct(\"playlist_id\").alias(\"total_playlists\"),\n",
    "        F.countDistinct(\"track_uri\").alias(\"total_tracks\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_data_quality.write.mode(\"overwrite\").saveAsTable(\"gold_data_quality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1ea431-c18b-499d-bf39-f22e1784f3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "gold_playlist_lifecycle = (\n",
    "    silver_playlists\n",
    "    .groupBy(\"playlist_id\")\n",
    "    .agg(\n",
    "        F.min(\"playlist_modified_ts\").alias(\"first_seen_date\"),\n",
    "        F.max(\"playlist_modified_ts\").alias(\"last_seen_date\"),\n",
    "        F.countDistinct(\"playlist_modified_ts\").alias(\"active_days\"),\n",
    "        F.min(\"track_rows\").alias(\"min_tracks\"),\n",
    "        F.max(\"track_rows\").alias(\"max_tracks\"),\n",
    "        F.round(F.avg(\"track_rows\"), 2).alias(\"avg_tracks\"),\n",
    "        F.first(\"followers\").alias(\"followers\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"track_growth\",\n",
    "        F.col(\"max_tracks\") - F.col(\"min_tracks\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_playlist_lifecycle.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"gold_playlist_lifecycle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a38a14-47ec-474f-b390-e02306e74182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "gold_track_lifecycle = (\n",
    "    silver_playlist_tracks\n",
    "    .groupBy(\"track_uri\")\n",
    "    .agg(\n",
    "        F.min(\"playlist_modified_at\").alias(\"first_seen_date\"),\n",
    "        F.max(\"playlist_modified_at\").alias(\"last_seen_date\"),\n",
    "        F.countDistinct(\"playlist_modified_at\").alias(\"active_days\"),\n",
    "        F.countDistinct(\"playlist_id\").alias(\"playlist_count\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_track_lifecycle.write.mode(\"overwrite\").saveAsTable(\"gold_track_lifecycle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c988b7-4343-4cd3-adab-dcd074630813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "daily_track_popularity = (\n",
    "    silver_playlist_tracks\n",
    "    .groupBy(\"playlist_modified_at\", \"track_uri\")\n",
    "    .agg(F.countDistinct(\"playlist_id\").alias(\"playlists_containing_track\"))\n",
    "    .withColumnRenamed(\"playlist_modified_at\", \"event_date\")\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"track_uri\").orderBy(\"event_date\")\n",
    "\n",
    "gold_track_trend_velocity = (\n",
    "    daily_track_popularity\n",
    "    .withColumn(\"prev_playlists_containing_track\", F.lag(\"playlists_containing_track\").over(w))\n",
    "    .withColumn(\n",
    "        \"delta_playlists\",\n",
    "        F.col(\"playlists_containing_track\") - F.col(\"prev_playlists_containing_track\")\n",
    "    )\n",
    "    .where(F.col(\"prev_playlists_containing_track\").isNotNull())\n",
    "    .orderBy(\"event_date\", \"track_uri\")\n",
    ")\n",
    "\n",
    "gold_track_trend_velocity.write.mode(\"overwrite\").saveAsTable(\"gold_track_trend_velocity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7044aa-d40f-450d-95d2-095c26340b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "artist_counts = (\n",
    "    silver_playlist_tracks\n",
    "    .groupBy(\"playlist_id\", \"artist_uri\")\n",
    "    .agg(F.count(\"*\").alias(\"track_count\"))\n",
    ")\n",
    "\n",
    "playlist_totals = (\n",
    "    artist_counts\n",
    "    .groupBy(\"playlist_id\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"artist_uri\").alias(\"unique_artists\"),\n",
    "        F.sum(\"track_count\").alias(\"total_tracks\"),\n",
    "        F.max(\"track_count\").alias(\"top_artist_tracks\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_playlist_artist_concentration = (\n",
    "    playlist_totals\n",
    "    .withColumn(\"top_artist_share\", F.col(\"top_artist_tracks\") / F.col(\"total_tracks\"))\n",
    "    .drop(\"top_artist_tracks\")\n",
    ")\n",
    "\n",
    "gold_playlist_artist_concentration.write.mode(\"overwrite\").saveAsTable(\"gold_playlist_artist_concentration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4f79d1-ce2d-4e83-a251-8979188bbd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "a = silver_playlist_tracks.select(\n",
    "    F.col(\"playlist_id\").alias(\"playlist_id\"),\n",
    "    F.col(\"playlist_modified_at\").alias(\"event_date\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri_1\"),\n",
    ")\n",
    "\n",
    "b = silver_playlist_tracks.select(\n",
    "    F.col(\"playlist_id\").alias(\"playlist_id\"),\n",
    "    F.col(\"playlist_modified_at\").alias(\"event_date\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri_2\"),\n",
    ")\n",
    "\n",
    "gold_track_pairs = (\n",
    "    a.join(\n",
    "        b,\n",
    "        on=[\n",
    "            a.playlist_id == b.playlist_id,\n",
    "            a.event_date == b.event_date\n",
    "        ],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .where(F.col(\"track_uri_1\") < F.col(\"track_uri_2\"))\n",
    "    .groupBy(\"track_uri_1\", \"track_uri_2\")\n",
    "    .agg(F.count(\"*\").alias(\"co_occurrence_count\"))\n",
    "    .orderBy(F.desc(\"co_occurrence_count\"))\n",
    ")\n",
    "\n",
    "gold_track_pairs.write.mode(\"overwrite\").saveAsTable(\"gold_track_pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74de0175-32ff-4d72-a56d-86d389753bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_playlist_artist_concentration updated with merged schema\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load silver playlist-track data\n",
    "silver_df = spark.table(\"silver_playlist_tracks\")\n",
    "\n",
    "# 1. Tracks per artist per playlist\n",
    "artist_counts = (\n",
    "    silver_df\n",
    "    .groupBy(\"playlist_id\", \"artist_name\")\n",
    "    .agg(F.count(\"*\").alias(\"n_tracks\"))\n",
    ")\n",
    "\n",
    "# 2. Total tracks per playlist\n",
    "playlist_totals = (\n",
    "    artist_counts\n",
    "    .groupBy(\"playlist_id\")\n",
    "    .agg(F.sum(\"n_tracks\").alias(\"total_tracks\"))\n",
    ")\n",
    "\n",
    "# 3. Artist share inside playlist\n",
    "shares = (\n",
    "    artist_counts\n",
    "    .join(playlist_totals, on=\"playlist_id\")\n",
    "    .withColumn(\"artist_share\", F.col(\"n_tracks\") / F.col(\"total_tracks\"))\n",
    ")\n",
    "\n",
    "# 4. Artist concentration metrics\n",
    "gold_df = (\n",
    "    shares\n",
    "    .groupBy(\"playlist_id\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"artist_name\").alias(\"total_artists\"),\n",
    "        F.max(\"artist_share\").alias(\"top_artist_share\"),\n",
    "        F.first(\"artist_name\", ignorenulls=True).alias(\"top_artist_name\"),\n",
    "        (F.lit(1) - F.sum(F.col(\"artist_share\") * F.col(\"artist_share\")))\n",
    "            .alias(\"gini_artist_index\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Write with schema merge (IMPORTANT FIX)\n",
    "(\n",
    "    gold_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"gold_playlist_artist_concentration\")\n",
    ")\n",
    "\n",
    "print(\"gold_playlist_artist_concentration updated with merged schema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b87b2a6-9f13-4535-9f60-288c937936f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_track_trend_velocity created successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load silver playlist-track data\n",
    "# ---------------------------------------\n",
    "silver_df = spark.table(\"silver_playlist_tracks\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Convert playlist_modified_at to timestamp\n",
    "#    (UNIX milliseconds → timestamp)\n",
    "# ---------------------------------------\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"modified_ts\",\n",
    "    F.to_timestamp(F.col(\"playlist_modified_at\") / 1000)\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Weekly playlist coverage per track\n",
    "# ---------------------------------------\n",
    "weekly_coverage = (\n",
    "    silver_df\n",
    "    .withColumn(\"week_start\", F.date_trunc(\"week\", F.col(\"modified_ts\")))\n",
    "    .groupBy(\n",
    "        \"track_uri\",\n",
    "        \"track_name\",\n",
    "        \"artist_name\",\n",
    "        \"week_start\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.countDistinct(\"playlist_id\").alias(\"playlist_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Week-over-week net change\n",
    "# ---------------------------------------\n",
    "w = Window.partitionBy(\"track_uri\").orderBy(\"week_start\")\n",
    "\n",
    "trend_velocity = (\n",
    "    weekly_coverage\n",
    "    .withColumn(\n",
    "        \"prev_playlist_count\",\n",
    "        F.lag(\"playlist_count\").over(w)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"net_change\",\n",
    "        F.col(\"playlist_count\") - F.coalesce(F.col(\"prev_playlist_count\"), F.lit(0))\n",
    "    )\n",
    "    .drop(\"prev_playlist_count\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Write Gold table (schema-safe)\n",
    "# ---------------------------------------\n",
    "(\n",
    "    trend_velocity\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"gold_track_trend_velocity\")\n",
    ")\n",
    "\n",
    "print(\"gold_track_trend_velocity created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7592edd4-ced7-4247-9acb-f0586f6fd259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_track_pairs created successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load silver playlist-track data\n",
    "# ---------------------------------------\n",
    "silver_df = spark.table(\"silver_playlist_tracks\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Self-join tracks within the same playlist\n",
    "#    (track A < track B to avoid duplicates)\n",
    "# ---------------------------------------\n",
    "pairs = (\n",
    "    silver_df.alias(\"a\")\n",
    "    .join(\n",
    "        silver_df.alias(\"b\"),\n",
    "        on=\"playlist_id\"\n",
    "    )\n",
    "    .where(F.col(\"a.track_uri\") < F.col(\"b.track_uri\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Count co-occurrences\n",
    "# ---------------------------------------\n",
    "co_occurrence = (\n",
    "    pairs\n",
    "    .groupBy(\n",
    "        F.col(\"a.track_uri\").alias(\"track_uri_1\"),\n",
    "        F.col(\"a.track_name\").alias(\"track_name_1\"),\n",
    "        F.col(\"a.artist_name\").alias(\"artist_name_1\"),\n",
    "        F.col(\"b.track_uri\").alias(\"track_uri_2\"),\n",
    "        F.col(\"b.track_name\").alias(\"track_name_2\"),\n",
    "        F.col(\"b.artist_name\").alias(\"artist_name_2\"),\n",
    "    )\n",
    "    .agg(F.countDistinct(\"playlist_id\").alias(\"co_occurrence_count\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Write Gold table (schema-safe)\n",
    "# ---------------------------------------\n",
    "(\n",
    "    co_occurrence\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"gold_track_pairs\")\n",
    ")\n",
    "\n",
    "print(\"old_track_pairs created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff2ef74-825c-4e76-a8ce-a5ffde9079a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "silver_playlists = spark.table(\"silver_playlists\")\n",
    "silver_playlist_tracks = spark.table(\"silver_playlist_tracks\")\n",
    "\n",
    "# =========================\n",
    "# gold_playlist_lifecycle (history-aware)\n",
    "# =========================\n",
    "\n",
    "# 1) Daily playlist sizes (1 row per playlist per day)\n",
    "daily_sizes = (\n",
    "    silver_playlist_tracks\n",
    "    .where(\n",
    "        F.col(\"playlist_id\").isNotNull()\n",
    "        & F.col(\"track_uri\").isNotNull()\n",
    "        & F.col(\"playlist_modified_at\").isNotNull()\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"event_ts\",\n",
    "        F.when(\n",
    "            F.col(\"playlist_modified_at\").cast(\"string\").rlike(\"^[0-9]{10}$\"),\n",
    "            F.to_timestamp(F.col(\"playlist_modified_at\").cast(\"long\"))\n",
    "        ).otherwise(\n",
    "            F.to_timestamp(\"playlist_modified_at\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    "    .groupBy(\"playlist_id\", \"event_date\")\n",
    "    .agg(F.countDistinct(\"track_uri\").alias(\"track_count\"))\n",
    ")\n",
    "\n",
    "# 2) Compute first/last day track_count per playlist, then net growth\n",
    "w_first = Window.partitionBy(\"playlist_id\").orderBy(F.col(\"event_date\").asc())\n",
    "w_last  = Window.partitionBy(\"playlist_id\").orderBy(F.col(\"event_date\").desc())\n",
    "\n",
    "daily_with_flags = (\n",
    "    daily_sizes\n",
    "    .withColumn(\"rn_first\", F.row_number().over(w_first))\n",
    "    .withColumn(\"rn_last\",  F.row_number().over(w_last))\n",
    "    .withColumn(\"first_day_tracks\", F.when(F.col(\"rn_first\") == 1, F.col(\"track_count\")))\n",
    "    .withColumn(\"last_day_tracks\",  F.when(F.col(\"rn_last\")  == 1, F.col(\"track_count\")))\n",
    ")\n",
    "\n",
    "lifecycle = (\n",
    "    daily_with_flags\n",
    "    .groupBy(\"playlist_id\")\n",
    "    .agg(\n",
    "        F.min(\"event_date\").alias(\"first_seen_date\"),\n",
    "        F.max(\"event_date\").alias(\"last_seen_date\"),\n",
    "        F.count(\"*\").alias(\"active_days\"),\n",
    "        F.min(\"track_count\").alias(\"min_tracks\"),\n",
    "        F.max(\"track_count\").alias(\"max_tracks\"),\n",
    "        F.avg(\"track_count\").alias(\"avg_tracks\"),\n",
    "        F.max(\"first_day_tracks\").alias(\"first_day_tracks\"),\n",
    "        F.max(\"last_day_tracks\").alias(\"last_day_tracks\"),\n",
    "    )\n",
    "    .withColumn(\"track_growth\", F.col(\"last_day_tracks\") - F.col(\"first_day_tracks\"))\n",
    "    .drop(\"first_day_tracks\", \"last_day_tracks\")\n",
    ")\n",
    "\n",
    "# 3) Latest followers per playlist (silver_playlists uses playlist_modified_ts + followers)\n",
    "w_followers = Window.partitionBy(\"playlist_id\").orderBy(F.col(\"playlist_modified_ts\").desc())\n",
    "\n",
    "followers_latest = (\n",
    "    silver_playlists\n",
    "    .where(\n",
    "        F.col(\"playlist_id\").isNotNull()\n",
    "        & F.col(\"playlist_modified_ts\").isNotNull()\n",
    "        & F.col(\"followers\").isNotNull()\n",
    "    )\n",
    "    .withColumn(\"rn\", F.row_number().over(w_followers))\n",
    "    .where(F.col(\"rn\") == 1)\n",
    "    .select(\n",
    "        \"playlist_id\",\n",
    "        F.col(\"followers\").cast(\"bigint\").alias(\"followers\")\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_playlist_lifecycle = (\n",
    "    lifecycle\n",
    "    .join(followers_latest, on=\"playlist_id\", how=\"left\")\n",
    "    .withColumn(\"followers\", F.coalesce(F.col(\"followers\"), F.lit(0)))\n",
    ")\n",
    "\n",
    "(\n",
    "  gold_playlist_lifecycle\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")\n",
    "  .saveAsTable(\"gold_playlist_lifecycle\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}